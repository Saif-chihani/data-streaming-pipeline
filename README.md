# Engagement Streaming Pipeline

Un systÃ¨me de streaming en temps rÃ©el pour traiter les Ã©vÃ©nements d'engagement utilisateur depuis PostgreSQL vers BigQuery, Redis et un systÃ¨me externe avec des transformations et enrichissements de donnÃ©es.

## ğŸ¯ Objectif

Ce projet implÃ©mente une pipeline de streaming de donnÃ©es qui :

- **Capture** les Ã©vÃ©nements d'engagement en temps rÃ©el depuis PostgreSQL
- **Enrichit** les donnÃ©es en joignant les mÃ©tadonnÃ©es de contenu
- **Transforme** les Ã©vÃ©nements (calcul de durÃ©es, pourcentages d'engagement)
- **Distribue** vers 3 destinations en parallÃ¨le (Fan-out Multi-sink)
- **Garantit** un traitement exactly-once
- **Fournit** des agrÃ©gations temps rÃ©el (Redis < 5 secondes)

## ğŸ—ï¸ Architecture

```
PostgreSQL (Source) 
     â†“ (CDC)
   Kafka Topic
     â†“
Stream Processor
     â†“ (Fan-out)
  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
  â†“     â†“     â†“     â†“
Redis BigQuery External Monitoring
```

### Composants

1. **PostgreSQL** : Base source avec tables `content` et `engagement_events`
2. **Kafka** : Message broker pour la capture de donnÃ©es changÃ©es (CDC)
3. **Stream Processor** : Application Python principale avec exactly-once processing
4. **Redis** : AgrÃ©gations temps rÃ©el (< 5 secondes)
5. **BigQuery** : Analytics et reporting
6. **SystÃ¨me Externe** : API REST pour intÃ©grations tierces
7. **Monitoring** : API de santÃ© et mÃ©triques Prometheus

## ğŸ“Š ModÃ¨le de DonnÃ©es

### Source (PostgreSQL)

```sql
-- Catalogue de contenu
CREATE TABLE content (
    id              UUID PRIMARY KEY,
    slug            TEXT UNIQUE NOT NULL,
    title           TEXT NOT NULL,
    content_type    TEXT CHECK (content_type IN ('podcast', 'newsletter', 'video')),
    length_seconds  INTEGER, 
    publish_ts      TIMESTAMPTZ NOT NULL
);

-- Ã‰vÃ©nements d'engagement bruts
CREATE TABLE engagement_events (
    id           BIGSERIAL PRIMARY KEY,
    content_id   UUID REFERENCES content(id),
    user_id      UUID,
    event_type   TEXT CHECK (event_type IN ('play', 'pause', 'finish', 'click')),
    event_ts     TIMESTAMPTZ NOT NULL,
    duration_ms  INTEGER,
    device       TEXT,
    raw_payload  JSONB
);
```

### Transformations

Pour chaque Ã©vÃ©nement, le systÃ¨me calcule :

- **engagement_seconds** = `duration_ms / 1000`
- **engagement_pct** = `(engagement_seconds / length_seconds) * 100`

## ğŸš€ Installation et DÃ©marrage

### PrÃ©requis

- Docker & Docker Compose
- Python 3.11+
- Minimum 8GB RAM
- Ports disponibles : 5432, 6379, 9092, 8083, 8080

### DÃ©marrage Rapide

1. **Cloner et configurer**
```bash
git clone <repository-url>
cd data-streaming-pipeline
cp .env.example .env
```

2. **Modifier la configuration**
```bash
# Ã‰ditez .env avec vos paramÃ¨tres
nano .env
```

3. **DÃ©marrer l'infrastructure**
```bash
# Linux/Mac
chmod +x start.sh
./start.sh

# Windows
docker-compose up -d
```

4. **VÃ©rifier le statut**
```bash
# SantÃ© des services
curl http://localhost:8080/health

# Top contenu en temps rÃ©el
curl http://localhost:8080/top-content
```

### Commandes Utiles

```bash
# Voir les logs
docker-compose logs -f stream-processor
docker-compose logs -f data-generator

# RedÃ©marrer un service
docker-compose restart stream-processor

# ArrÃªter tous les services
docker-compose down

# Nettoyer les volumes (âš ï¸ perte de donnÃ©es)
docker-compose down -v
```

## âš™ï¸ Configuration

### Variables d'Environnement Principales

```bash
# Base de donnÃ©es
DATABASE_URL=postgresql://postgres:postgres@localhost:5432/engagement_db

# Kafka
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
KAFKA_TOPIC_ENGAGEMENT_EVENTS=engagement-events

# Redis
REDIS_URL=redis://localhost:6379
REDIS_AGGREGATION_WINDOW_MINUTES=10

# BigQuery
BIGQUERY_PROJECT_ID=your-project-id
BIGQUERY_DATASET_ID=engagement_analytics

# Traitement
BATCH_SIZE=100
PROCESSING_INTERVAL_SECONDS=1
ENABLE_EXACTLY_ONCE=true
```

### Configuration BigQuery

1. CrÃ©er un projet GCP
2. Activer l'API BigQuery
3. CrÃ©er une clÃ© de service
4. TÃ©lÃ©charger le fichier JSON dans `config/bigquery-credentials.json`

## ğŸ”„ Modes d'ExÃ©cution

### Mode Streaming (Temps RÃ©el)

```bash
docker exec -it stream_processor python -m src.stream_processor --mode stream
```

### Mode Backfill (DonnÃ©es Historiques)

```bash
docker exec -it stream_processor python -m src.stream_processor \
  --mode backfill \
  --start-date 2025-08-01 \
  --end-date 2025-08-10
```

### GÃ©nÃ©ration de DonnÃ©es

```bash
# Mode continu
docker exec -it data_generator python -m src.data_generator --mode continuous

# Batch unique
docker exec -it data_generator python -m src.data_generator --mode batch --batch-size 100

# DonnÃ©es historiques
docker exec -it data_generator python -m src.data_generator --mode historical --historical-days 7
```

## ğŸ“ˆ Monitoring et MÃ©triques

### API de SantÃ©

- **SantÃ© globale** : `GET /health`
- **MÃ©triques systÃ¨me** : `GET /metrics`
- **Prometheus** : `GET /metrics/prometheus`
- **Top contenu** : `GET /top-content`
- **Stats contenu** : `GET /content/{id}/stats`

### Redis - DonnÃ©es Temps RÃ©el

```python
# Contenu le plus engageant (10 derniÃ¨res minutes)
top_content = await redis_sink.get_top_content(limit=10)

# Stats spÃ©cifiques Ã  un contenu
stats = await redis_sink.get_content_stats(content_id)

# Ã‰vÃ©nements rÃ©cents
events = await redis_sink.get_recent_events(content_id, count=50)
```

### BigQuery - Analytics

```sql
-- Vue quotidienne d'engagement
SELECT * FROM `project.dataset.daily_engagement_summary`
WHERE event_date >= CURRENT_DATE() - 7;

-- Tendances horaires
SELECT * FROM `project.dataset.hourly_engagement_trends`
WHERE hour_bucket >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 24 HOUR);
```

## ğŸ”§ DÃ©veloppement

### Structure du Projet

```
data-streaming-pipeline/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models.py              # ModÃ¨les de donnÃ©es Pydantic
â”‚   â”œâ”€â”€ config.py              # Configuration centralisÃ©e
â”‚   â”œâ”€â”€ data_generator.py      # GÃ©nÃ©rateur de donnÃ©es de test
â”‚   â”œâ”€â”€ stream_processor.py    # Processeur principal
â”‚   â”œâ”€â”€ monitoring.py          # API de monitoring
â”‚   â””â”€â”€ sinks/
â”‚       â”œâ”€â”€ redis_sink.py      # Sink Redis
â”‚       â”œâ”€â”€ bigquery_sink.py   # Sink BigQuery
â”‚       â””â”€â”€ external_sink.py   # Sink systÃ¨me externe
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ init.sql              # SchÃ©ma et donnÃ©es initiales
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.generator   # Image gÃ©nÃ©rateur
â”‚   â””â”€â”€ Dockerfile.processor   # Image processeur
â”œâ”€â”€ config/
â”‚   â””â”€â”€ bigquery-credentials.json
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### Tests

```bash
# Tests unitaires
docker exec -it stream_processor python -m pytest tests/

# Test de charge
docker exec -it data_generator python -m src.data_generator \
  --mode batch --batch-size 1000
```

### Personnalisation

1. **Nouveau sink** : Ã‰tendre la classe de base dans `sinks/`
2. **Transformations** : Modifier `enrich_event()` dans `stream_processor.py`
3. **MÃ©triques** : Ajouter des endpoints dans `monitoring.py`

## ğŸ¯ Performances et Garanties

### Latence
- **Redis** : < 5 secondes (objectif atteint)
- **BigQuery** : Batch 30 secondes max
- **SystÃ¨me externe** : < 30 secondes avec retry

### DÃ©bit
- **Production** : 1000+ Ã©vÃ©nements/seconde
- **Backfill** : 10,000+ Ã©vÃ©nements/seconde

### Garanties
- **Exactly-once processing** avec transactions Kafka
- **Idempotence** sur tous les sinks
- **Retry automatique** avec backoff exponentiel
- **Dead letter queues** pour les Ã©checs persistants

## ğŸ” DÃ©pannage

### ProblÃ¨mes Courants

1. **Kafka non dÃ©marrÃ©**
```bash
docker-compose logs kafka
# VÃ©rifier les ports et Zookeeper
```

2. **BigQuery credentials manquantes**
```bash
# VÃ©rifier le fichier de credentials
ls -la config/bigquery-credentials.json
```

3. **Redis connexion refusÃ©e**
```bash
docker-compose logs redis
# VÃ©rifier si Redis est dÃ©marrÃ©
```

4. **Processeur bloquÃ©**
```bash
# Logs dÃ©taillÃ©s
docker-compose logs -f stream-processor
# RedÃ©marrer si nÃ©cessaire
docker-compose restart stream-processor
```

### Logs Utiles

```bash
# Logs en temps rÃ©el
docker-compose logs -f

# Logs d'un service spÃ©cifique
docker-compose logs -f stream-processor

# Logs avec timestamps
docker-compose logs -f -t
```

## ğŸ“‹ Roadmap et AmÃ©liorations

### ImplÃ©mentÃ© âœ…
- [x] Streaming temps rÃ©el avec Kafka
- [x] Exactly-once processing
- [x] Fan-out vers 3 destinations
- [x] AgrÃ©gations Redis < 5 secondes
- [x] Backfill pour donnÃ©es historiques
- [x] Monitoring et mÃ©triques
- [x] Docker containerization

### Prochaines Ã‰tapes ğŸš§
- [ ] Schema Registry pour Ã©volution des donnÃ©es
- [ ] Kafka Streams pour processing distribuÃ©
- [ ] Auto-scaling basÃ© sur la charge
- [ ] Tests d'intÃ©gration end-to-end
- [ ] Dashboards Grafana
- [ ] Alerting automatique
- [ ] Chiffrement des donnÃ©es sensibles

### Optimisations Possibles ğŸ“ˆ
- [ ] Partitioning intelligent par content_type
- [ ] Compression des payloads Kafka
- [ ] Connection pooling optimisÃ©
- [ ] Cache intelligent pour les jointures
- [ ] Batch processing adaptatif

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commit les changements (`git commit -m 'Add AmazingFeature'`)
4. Push vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

## ğŸ‘¨â€ğŸ’» Auteur

DÃ©veloppÃ© dans le cadre d'un test technique pour un poste d'ingÃ©nieur de donnÃ©es senior.

**Technologies utilisÃ©es** : Python, Kafka, PostgreSQL, Redis, BigQuery, Docker, FastAPI, Pydantic
